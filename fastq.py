# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gYhMmnOIuvnzQWy824iGrYY7kPYfpk1e
"""

# prompt: 🔹 Rol:
# Eres un desarrollador bioinformático experto en visualización de datos genómicos y en la creación de herramientas interactivas en Python con Streamlit.
# 🔹 Instrucción:
# Crea una aplicación de control de calidad para archivos de secuenciación en formato FASTQ utilizando Python y Streamlit.
# Primero, implementa todas las funciones necesarias para calcular las siguientes métricas a partir del contenido de un archivo FASTQ:
# Calidad por posición: calcula la calidad media (Phred) base por base a lo largo de todas las lecturas.
# Calidad media por lectura: calcula el promedio de calidad por secuencia.
# Contenido GC: porcentaje de bases G y C sobre el total.
# Contenido por base: proporción de A, T, G y C en cada posición.
# Distribución de longitudes: muestra la longitud de cada lectura y su distribución.
# Secuencias duplicadas: cuenta cuántas secuencias están repetidas y con qué frecuencia.
# Secuencias sobre-representadas: muestra las secuencias que más veces se repiten.
# Presencia de adaptadores: detecta si hay adaptadores comunes no eliminados.
# Luego, crea una aplicación Streamlit que:
# Permita cargar un archivo FASTQ desde la interfaz.
# Ejecute los cálculos anteriores.
# Muestre los resultados de forma organizada (por secciones o pestañas).
# Utilice visualizaciones con matplotlib, seaborn o plotly.
# Tenga una interfaz clara y atractiva.
# 🔹 Contexto:
# Esta aplicación está pensada como una herramienta sencilla pero poderosa para investigadores que necesitan analizar archivos FASTQ sin depender de plataformas complejas como FastQC. Debe correr localmente usando streamlit run app.py, sin depender de conexión a internet. Los análisis deben estar optimizados para archivos pequeños o medianos (hasta 50 MB).
# 🔹 Formato de salida:
# Entrega un solo archivo llamado app.py que:
# Contenga todas las funciones de análisis.
# Incluya la interfaz completa en Streamlit.
# Use bibliotecas estándar de Python (como collections, matplotlib, plotly, pandas, etc.).
# No dependa de archivos

import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict, Counter
import plotly.graph_objects as go
import plotly.express as px
import re

# --- Funciones de análisis ---

def parse_fastq(file):
    """Parses a FASTQ file and yields sequences and quality strings."""
    name, seq, plus, qual = None, None, None, None
    for i, line in enumerate(file):
        line = line.decode('utf-8').strip()
        if i % 4 == 0:
            name = line
        elif i % 4 == 1:
            seq = line
        elif i % 4 == 2:
            plus = line
        elif i % 4 == 3:
            qual = line
            yield (name, seq, plus, qual)
            name, seq, plus, qual = None, None, None, None

def calculate_quality_per_position(fastq_content):
    """Calculates the average Phred quality score per position."""
    qualities_by_position = defaultdict(list)
    total_reads = 0
    for name, seq, plus, qual in parse_fastq(fastq_content):
        total_reads += 1
        for i, q_char in enumerate(qual):
            qualities_by_position[i].append(ord(q_char) - 33) # Phred+33 encoding

    avg_qualities = {pos: sum(quals) / len(quals) for pos, quals in qualities_by_position.items()}
    return avg_qualities, total_reads

def calculate_average_quality_per_read(fastq_content):
    """Calculates the average quality score for each read."""
    avg_qualities = []
    for name, seq, plus, qual in parse_fastq(fastq_content):
        read_qualities = [ord(q_char) - 33 for q_char in qual]
        if read_qualities:
            avg_qualities.append(sum(read_qualities) / len(read_qualities))
    return avg_qualities

def calculate_gc_content(fastq_content):
    """Calculates the overall GC content."""
    total_bases = 0
    gc_bases = 0
    for name, seq, plus, qual in parse_fastq(fastq_content):
        total_bases += len(seq)
        gc_bases += seq.count('G') + seq.count('C')
    return (gc_bases / total_bases) * 100 if total_bases > 0 else 0

def calculate_base_content_per_position(fastq_content):
    """Calculates the proportion of A, T, G, C at each position."""
    base_counts = defaultdict(lambda: defaultdict(int)) # position -> base -> count
    total_reads = 0
    for name, seq, plus, qual in parse_fastq(fastq_content):
        total_reads += 1
        for i, base in enumerate(seq):
            base_counts[i][base] += 1

    base_proportions = {}
    for pos, counts in base_counts.items():
        total_at_pos = sum(counts.values())
        base_proportions[pos] = {base: count / total_at_pos for base, count in counts.items()}
    return base_proportions

def analyze_read_lengths(fastq_content):
    """Analyzes the distribution of read lengths."""
    lengths = []
    for name, seq, plus, qual in parse_fastq(fastq_content):
        lengths.append(len(seq))
    return lengths

def find_duplicate_sequences(fastq_content):
    """Counts the frequency of duplicate sequences."""
    sequences = Counter()
    for name, seq, plus, qual in parse_fastq(fastq_content):
        sequences[seq] += 1
    return sequences

def find_overrepresented_sequences(sequences_counts, threshold=0.1):
    """Finds sequences that are overrepresented (e.g., > 0.1% of total reads)."""
    total_reads = sum(sequences_counts.values())
    overrepresented = {seq: count for seq, count in sequences_counts.items() if count / total_reads > threshold and count > 1}
    return overrepresented

def detect_adapters(fastq_content, adapter_sequences):
    """Detects the presence of common adapter sequences."""
    adapter_counts = defaultdict(int)
    total_reads = 0
    for name, seq, plus, qual in parse_fastq(fastq_content):
        total_reads += 1
        for adapter_name, adapter_seq in adapter_sequences.items():
            if adapter_seq in seq:
                adapter_counts[adapter_name] += 1
    return adapter_counts, total_reads

# --- Streamlit App ---

st.title("FASTQ Quality Control App")

st.sidebar.header("Upload FASTQ File")
uploaded_file = st.sidebar.file_uploader("Choose a .fastq file", type="fastq")

if uploaded_file is not None:
    # Read the file content
    fastq_content = uploaded_file.getvalue()

    st.header("Analysis Results")

    # Quality per Position
    st.subheader("Quality per Position")
    with st.spinner("Calculating quality per position..."):
        avg_qualities_pos, total_reads = calculate_quality_per_position(fastq_content)
        if avg_qualities_pos:
            positions = sorted(avg_qualities_pos.keys())
            qualities = [avg_qualities_pos[pos] for pos in positions]
            plt.figure(figsize=(12, 6))
            plt.plot(positions, qualities)
            plt.xlabel("Position in read")
            plt.ylabel("Average Quality Score (Phred)")
            plt.title("Average Quality Score Across Read Positions")
            plt.grid(True)
            st.pyplot(plt)
            st.write(f"Analyzed {total_reads} reads.")
        else:
            st.warning("No quality data found.")

    # Average Quality per Read
    st.subheader("Average Quality per Read")
    with st.spinner("Calculating average quality per read..."):
        avg_qualities_read = calculate_average_quality_per_read(fastq_content)
        if avg_qualities_read:
            plt.figure(figsize=(10, 6))
            sns.histplot(avg_qualities_read, bins=30, kde=True)
            plt.xlabel("Average Quality Score (Phred)")
            plt.ylabel("Number of Reads")
            plt.title("Distribution of Average Read Quality")
            st.pyplot(plt)
        else:
            st.warning("No average quality data found.")

    # GC Content
    st.subheader("GC Content")
    with st.spinner("Calculating GC content..."):
        gc_percentage = calculate_gc_content(fastq_content)
        st.metric(label="Overall GC Content", value=f"{gc_percentage:.2f}%")

    # Base Content per Position
    st.subheader("Base Content per Position")
    with st.spinner("Calculating base content per position..."):
        base_proportions = calculate_base_content_per_position(fastq_content)
        if base_proportions:
            df_bases = pd.DataFrame(base_proportions).T.fillna(0)
            df_bases = df_bases[['A', 'T', 'G', 'C']].sort_index()

            fig = go.Figure()
            for base in ['A', 'T', 'G', 'C']:
                fig.add_trace(go.Scatter(x=df_bases.index, y=df_bases[base], mode='lines', name=base, stackgroup='one'))

            fig.update_layout(
                title='Base Content Distribution Across Read Positions',
                xaxis_title='Position in Read',
                yaxis_title='Proportion',
                hovermode='x unified'
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("No base content data found.")

    # Distribution of Lengths
    st.subheader("Distribution of Read Lengths")
    with st.spinner("Analyzing read lengths..."):
        read_lengths = analyze_read_lengths(fastq_content)
        if read_lengths:
            plt.figure(figsize=(10, 6))
            sns.histplot(read_lengths, bins=sorted(list(set(read_lengths))), kde=False)
            plt.xlabel("Read Length")
            plt.ylabel("Number of Reads")
            plt.title("Distribution of Read Lengths")
            st.pyplot(plt)
            length_counts = Counter(read_lengths)
            st.write("Summary of lengths:")
            for length, count in sorted(length_counts.items()):
                 st.write(f"- Length {length}: {count} reads")
        else:
            st.warning("No read length data found.")


    # Duplicate Sequences
    st.subheader("Duplicate Sequences")
    with st.spinner("Identifying duplicate sequences..."):
        sequences_counts = find_duplicate_sequences(fastq_content)
        total_reads = sum(sequences_counts.values())
        if sequences_counts:
            duplicates = {seq: count for seq, count in sequences_counts.items() if count > 1}
            if duplicates:
                st.write(f"Found {len(duplicates)} unique duplicated sequences.")
                st.write(f"{sum(duplicates.values())} reads are duplicates (out of {total_reads} total reads).")
                st.dataframe(pd.DataFrame.from_dict(duplicates, orient='index', columns=['Count']).sort_values(by='Count', ascending=False).head(20), use_container_width=True)
            else:
                st.info("No duplicate sequences found.")
        else:
            st.warning("No sequence data found.")

    # Overrepresented Sequences
    st.subheader("Overrepresented Sequences")
    with st.spinner("Identifying overrepresented sequences..."):
        # Re-calculate counts if not available (e.g., user skipped duplicates section)
        if 'sequences_counts' not in locals():
             sequences_counts = find_duplicate_sequences(fastq_content)
             total_reads = sum(sequences_counts.values())

        if sequences_counts and total_reads > 0:
            overrepresented_sequences = find_overrepresented_sequences(sequences_counts)
            if overrepresented_sequences:
                st.write("Sequences that represent more than 0.1% of total reads:")
                df_over = pd.DataFrame.from_dict(overrepresented_sequences, orient='index', columns=['Count'])
                df_over['Percentage (%)'] = (df_over['Count'] / total_reads) * 100
                st.dataframe(df_over.sort_values(by='Count', ascending=False), use_container_width=True)
            else:
                st.info("No sequences found that are overrepresented (> 0.1% and > 1 read).")
        else:
            st.warning("Not enough sequence data to identify overrepresented sequences.")

    # Adapter Presence
    st.subheader("Adapter Presence")
    st.write("Note: This analysis checks for the exact match of common adapter sequences.")
    common_adapters = {
        "Illumina Universal Adapter": "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA",
        "Illumina RNA Adapter T7": "AATGATACGGCGACCACCGAGATCTACAC",
        "TruSeq Adapter, Index 1": "GATCGGAAGAGCACACGTCTGAACTCCAGTCACATCACGATCTCGTATGCCGTCTTCTGCTTG",
        # Add more common adapters here if needed
    }
    with st.spinner("Detecting common adapters..."):
        adapter_counts, total_reads_adapters = detect_adapters(fastq_content, common_adapters)
        if adapter_counts and total_reads_adapters > 0:
            st.write("Reads containing common adapters:")
            adapter_data = [{"Adapter": name, "Reads Found": count, "Percentage (%)": (count / total_reads_adapters) * 100} for name, count in adapter_counts.items()]
            st.dataframe(pd.DataFrame(adapter_data), use_container_width=True)
        else:
            st.info("No common adapters found in the reads or no reads processed.")


else:
    st.info("Please upload a FASTQ file to get started.")